Epoch = 2 train loss = 0.821888185599271
Total time predicting in infodump = 0.00022268295288085938
Total time predicting in training = 0.23949456214904785
Total time predicting in loss = 3.8736283779144287
Total time predicting in metrics = 3.109222888946533
Total time predicting in total_time_spent_on_backward = 3.3597347736358643
Total time predicting in total_time_spent_on_backward = 9.225403785705566
Training finished in 21.680946350097656 seconds 

batch size = 8
Epoch = 0 train loss = 0.7698096201491
Total time predicting in infodump = 0.0002243518829345703
Total time predicting in training = 1.9864366054534912
Total time predicting in loss = 2.308164596557617
Total time predicting in metrics = 4.019528388977051
Total time predicting in backward = 2.2023801803588867
Total time predicting in step = 12.238869667053223
Training finished in 29.74395251274109 seconds 

params = {'input_space': 'RGB', 'input_range': [0, 1], 
'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}
preprocess input = mean = [0.485, 0.456, 0.406],
std = [0.229, 0.224, 0.225], input_space = RGB, input_range = [0, 1], 


Epoch = 0 valid loss = 0.9585258811712265
Epoch = 0 train loss = 0.9234215424341314
Training finished in 22.421659231185913 seconds 

Epoch = 1 valid loss = 0.8815984427928925
Epoch = 1 train loss = 0.8420484048478744
Training finished in 21.718680143356323 seconds 

Epoch = 2 valid loss = 0.8239574730396271
Epoch = 2 train loss = 0.8006125320406521
Training finished in 21.63101315498352 seconds 


Epoch = 0 valid loss = 0.9585258811712265
Epoch = 0 train loss = 0.9234761069802677
Training finished in 18.43318772315979 seconds 


Epoch = 1 valid loss = 0.8800710886716843
Epoch = 1 train loss = 0.8408477394019856
Training finished in 18.412322282791138 seconds 


Epoch = 2 valid loss = 0.8303760588169098
Epoch = 2 train loss = 0.800149901824839
Training finished in 18.041751384735107 seconds


0.9744523763656616
0.9675304889678955
0.9253347516059875
0.9672038555145264
0.9399233460426331
0.9795821905136108
0.9352600574493408
0.9102529287338257
0.9249345064163208
0.9645532965660095
0.9227350950241089
0.9767346382141113
0.9281150102615356
0.9115477800369263
0.9609375596046448
0.9408515095710754
0.9220278263092041
0.9401764869689941
0.8772308826446533
0.9270384907722473
0.9208881258964539
0.9590279459953308
0.9257648587226868
0.9387093186378479
0.9569298028945923
0.9034584760665894
0.9697134494781494
0.9134536385536194
0.9462352395057678
0.9139009118080139
0.8796475529670715
0.872057318687439
0.932752251625061
0.9659625887870789

0.9225414395332336
0.9086069464683533
0.841566264629364
0.9156405329704285
0.8614249229431152
0.9464488625526428
0.8717636466026306
0.8244262337684631
0.8342618942260742
0.9069560170173645
0.8298487067222595
0.9315130710601807
0.860200822353363
0.8243006467819214
0.8994221687316895
0.863163948059082
0.8421754837036133
0.8653894662857056
0.7716884613037109
0.8353050351142883
0.8344136476516724
0.8930407762527466
0.8463495373725891
0.8660502433776855
0.9069673418998718
0.8158941268920898
0.902262806892395
0.8184914588928223
0.8747463226318359
0.8393425941467285
0.7961954474449158
0.7909586429595947
0.8577269315719604
0.9027839303016663